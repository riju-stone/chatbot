Papers:
	http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ (RNN Series)
	http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/ (Unfolding RNNs Part 1)
	http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/ (Unfolding RNNs Part 2)
	http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (Understanding LSTMs)
	http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (The Unreasonable Effectiveness of Recurrent Neural Networks)
	https://arxiv.org/pdf/1409.3215.pdf (Sequence to Sequence Learning with Neural Networks)
	https://arxiv.org/pdf/1406.1078.pdf (Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation)
	https://arxiv.org/pdf/1706.03762.pdf (Attention Is All You Need)
	https://arxiv.org/pdf/1409.0473.pdf (NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE)
	http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/ (Chatbots with Seq2Seq Part 1)
	http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/ (Practical seq2seq Part 2)
	https://jalammar.github.io/illustrated-transformer/ (The Illustrated Transformer)
	https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
	https://medium.com/swlh/what-exactly-is-happening-inside-the-transformer-b7f713d7aded
	https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
	https://towardsdatascience.com/attention-is-all-you-need-e498378552f9

Lectures:
	https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6
	https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S
	https://www.youtube.com/watch?v=S27pHKBEp30
	https://www.youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE

Videos:
	https://www.youtube.com/watch?v=dichIcUZfOw
	