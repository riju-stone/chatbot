{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial trains a <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a> to be a chatbot. This is an advanced example that assumes knowledge of [text generation](https://tensorflow.org/alpha/tutorials/text/text_generation), [attention](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) and [transformers](https://www.tensorflow.org/alpha/tutorials/text/transformer).\r\n",
    "\r\n",
    "The core idea behind the Transformer model is *self-attention*â€”the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections *Scaled dot product attention* and *Multi-head attention*.\r\n",
    "\r\n",
    "Note: The model architecture is identical to the example in [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer), and we demonstrate how to implement the same model in the Functional approach instead of Subclassing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
    "import sys\r\n",
    "!pip install tensorflow\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "!pip install tensorflow-datasets\r\n",
    "import tensorflow_datasets as tfds\r\n",
    "\r\n",
    "import os\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "from time import time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.40.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: keras~=2.6 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (3.17.3)\n",
      "Collecting attrs>=18.1.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (4.62.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (2.25.1)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (1.19.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (0.13.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\raka2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.4)\n",
      "Building wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=234a36e554cc430ca257a17769806bb316eaddc90898132319368dceef680a80\n",
      "  Stored in directory: c:\\users\\raka2\\appdata\\local\\pip\\cache\\wheels\\2f\\a0\\d3\\4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=a90a05eda50899395b3ba6f9497ee4b65d0b3532c21a655e57aac931a9aa0b22\n",
      "  Stored in directory: c:\\users\\raka2\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built future promise\n",
      "Installing collected packages: googleapis-common-protos, absl-py, tensorflow-metadata, promise, future, dill, attrs, tensorflow-datasets\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "Successfully installed absl-py-0.12.0 attrs-21.2.0 dill-0.3.4 future-0.18.2 googleapis-common-protos-1.53.0 promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.2.0\n",
      "Tensorflow version 2.6.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU / TPU initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "try:\r\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
    "    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\r\n",
    "except ValueError:\r\n",
    "    tpu = None\r\n",
    "\r\n",
    "if tpu:\r\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
    "else:\r\n",
    "    strategy = tf.distribute.get_strategy()\r\n",
    "\r\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPLICAS: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters\r\n",
    "\r\n",
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and units* have been reduced. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Maximum sentence length\r\n",
    "MAX_LENGTH = 40\r\n",
    "\r\n",
    "# Maximum number of samples to preprocess\r\n",
    "MAX_SAMPLES = 50000\r\n",
    "\r\n",
    "# For tf.data.Dataset\r\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\r\n",
    "BUFFER_SIZE = 20000\r\n",
    "\r\n",
    "# For Transformer\r\n",
    "NUM_LAYERS = 2\r\n",
    "D_MODEL = 256\r\n",
    "NUM_HEADS = 8\r\n",
    "UNITS = 512\r\n",
    "DROPOUT = 0.1\r\n",
    "\r\n",
    "EPOCHS = 40"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dataset\r\n",
    "\r\n",
    "We will use the conversations in movies and TV shows provided by [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.\r\n",
    "\r\n",
    "`movie_conversations.txt` contains list of the conversation IDs and `movie_lines.text` contains the text of assoicated with each conversation ID. For further  information regarding the dataset, please check the README file in the zip file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\r\n",
    "    'cornell_movie_dialogs.zip',\r\n",
    "    origin=\r\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\r\n",
    "    extract=True)\r\n",
    "\r\n",
    "path_to_dataset = os.path.join(\r\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\r\n",
    "\r\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\r\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\r\n",
    "                                           'movie_conversations.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
      "9920512/9916637 [==============================] - 3s 0us/step\n",
      "9928704/9916637 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and preprocess data\r\n",
    "\r\n",
    "To keep this example simple and fast, we are limiting the maximum number of training samples to`MAX_SAMPLES=25000` and the maximum length of the sentence to be `MAX_LENGTH=40`.\r\n",
    "\r\n",
    "We preprocess our dataset in the following order:\r\n",
    "* Extract `MAX_SAMPLES` conversation pairs into list of `questions` and `answers.\r\n",
    "* Preprocess each sentence by removing special characters in each sentence.\r\n",
    "* Build tokenizer (map text to ID and ID to text) using [TensorFlow Datasets SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder).\r\n",
    "* Tokenize each sentence and add `START_TOKEN` and `END_TOKEN` to indicate the start and end of each sentence.\r\n",
    "* Filter out sentence that has more than `MAX_LENGTH` tokens.\r\n",
    "* Pad tokenized sentences to `MAX_LENGTH`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def preprocess_sentence(sentence):\r\n",
    "  sentence = sentence.lower().strip()\r\n",
    "  # creating a space between a word and the punctuation following it\r\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\r\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\r\n",
    "  # removing contractions\r\n",
    "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\r\n",
    "  sentence = re.sub(r\"he's\", \"he is\", sentence)\r\n",
    "  sentence = re.sub(r\"she's\", \"she is\", sentence)\r\n",
    "  sentence = re.sub(r\"it's\", \"it is\", sentence)\r\n",
    "  sentence = re.sub(r\"that's\", \"that is\", sentence)\r\n",
    "  sentence = re.sub(r\"what's\", \"that is\", sentence)\r\n",
    "  sentence = re.sub(r\"where's\", \"where is\", sentence)\r\n",
    "  sentence = re.sub(r\"how's\", \"how is\", sentence)\r\n",
    "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\r\n",
    "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\r\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\r\n",
    "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\r\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\r\n",
    "  sentence = re.sub(r\"won't\", \"will not\", sentence)\r\n",
    "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\r\n",
    "  sentence = re.sub(r\"n't\", \" not\", sentence)\r\n",
    "  sentence = re.sub(r\"n'\", \"ng\", sentence)\r\n",
    "  sentence = re.sub(r\"'bout\", \"about\", sentence)\r\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\r\n",
    "  sentence = sentence.strip()\r\n",
    "  return sentence\r\n",
    "\r\n",
    "\r\n",
    "def load_conversations():\r\n",
    "  # dictionary of line id to text\r\n",
    "  id2line = {}\r\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\r\n",
    "    lines = file.readlines()\r\n",
    "  for line in lines:\r\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
    "    id2line[parts[0]] = parts[4]\r\n",
    "\r\n",
    "  inputs, outputs = [], []\r\n",
    "  with open(path_to_movie_conversations, 'r') as file:\r\n",
    "    lines = file.readlines()\r\n",
    "  for line in lines:\r\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
    "    # get conversation in a list of line ID\r\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\r\n",
    "    for i in range(len(conversation) - 1):\r\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\r\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\r\n",
    "      if len(inputs) >= MAX_SAMPLES:\r\n",
    "        return inputs, outputs\r\n",
    "  return inputs, outputs\r\n",
    "\r\n",
    "\r\n",
    "questions, answers = load_conversations()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Build tokenizer using tfds for both questions and answers\r\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\r\n",
    "    questions + answers, target_vocab_size=2**13)\r\n",
    "\r\n",
    "# Define start and end token to indicate the start and end of a sentence\r\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\r\n",
    "\r\n",
    "# Vocabulary size plus start and end token\r\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Tokenize, filter and pad sentences\r\n",
    "def tokenize_and_filter(inputs, outputs):\r\n",
    "  tokenized_inputs, tokenized_outputs = [], []\r\n",
    "  \r\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\r\n",
    "    # tokenize sentence\r\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\r\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\r\n",
    "    # check tokenized sentence max length\r\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\r\n",
    "      tokenized_inputs.append(sentence1)\r\n",
    "      tokenized_outputs.append(sentence2)\r\n",
    "  \r\n",
    "  # pad tokenized sentences\r\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\r\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\r\n",
    "  \r\n",
    "  return tokenized_inputs, tokenized_outputs\r\n",
    "\r\n",
    "\r\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create `tf.data.Dataset`\r\n",
    "\r\n",
    "We are going to use the [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data) to contruct our input pipline in order to utilize features like caching and prefetching to speed up the training process.\r\n",
    "\r\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.\r\n",
    "\r\n",
    "During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\r\n",
    "\r\n",
    "As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\r\n",
    "\r\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask.\r\n",
    "\r\n",
    "Target is divided into `decoder_inputs` which padded as an input to the decoder and `cropped_targets` for calculating our loss and accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "bf4dc6d344b952d756644de817bd34180bf4dc346a1eded414a4dd9f51dca411"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}